{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer Perceptron with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid Activation function for hidden layer \n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    \"\"\"\n",
    "    Derivative of Sigmoid function \n",
    "    \"\"\"\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def softmax(A):\n",
    "    \"\"\"\n",
    "    Softmax activation funciton for output layer\n",
    "    \"\"\"\n",
    "    e = np.exp(A)\n",
    "    return e / np.sum(e, axis=1, keepdims=True)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    if not (len(y_true) == len(y_pred)):\n",
    "        print('Size of predicted and true labels not equal.')\n",
    "        return 0.0\n",
    "\n",
    "    corr = 0\n",
    "    for i in range(0,len(y_true)):\n",
    "        corr += 1 if (y_true[i] == y_pred[i]).all() else 0\n",
    "\n",
    "    return corr/len(y_true)\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\"\n",
    "    Multi_layer perceptron:\n",
    "    - One hidden layer\n",
    "    - User defined number of Neurons\n",
    "    - User defined learning rate\n",
    "    - Hidden layer Activation function : Sigmoid\n",
    "    - Output layer Activation function : Softmax\n",
    "    - Cost function: Cross Entropy loss function\n",
    "    - Using Mini-batch gradient descent optimization technique\n",
    "    - User defined Batch size\n",
    "    \"\"\"\n",
    "    def __init__(self,inputSize,hiddenSize,outSize,lr):\n",
    "        \"\"\"\n",
    "        Initialize starting variables\n",
    "        \"\"\"\n",
    "        self.learning_rate=lr # set learning rate\n",
    "        # initialize layer dimensions \n",
    "        self.inputSize = inputSize # No of features of input data point \n",
    "        self.outSize = outSize # No of classes in label\n",
    "        self.hiddenSize = hiddenSize # No of Neurons in hidden layer \n",
    "        \n",
    "        #initialize parameters\n",
    "        np.random.seed(1) \n",
    "        self.W1 = np.zeros((self.inputSize,self.hiddenSize)) # intial Weights for input layer\n",
    "        self.b1 = np.zeros((1,self.hiddenSize)) # intial Bias for hidden layer\n",
    "        self.W2 = np.random.randn(self.hiddenSize,self.outSize) # intial Weights for hidden layer\n",
    "        self.b2 = np.zeros((1,self.outSize)) # intial Bias for out layer\n",
    " \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Get method to return parameter values and shape\n",
    "        \"\"\"\n",
    "        print(\"Weight_1:{}\\n{}\".format(self.W1.shape,self.W1))\n",
    "        print(\"Bias_1:{}\\n{}\".format(self.b1.shape,self.b1))\n",
    "        print(\"Weight_2:{}\\n{}\".format(self.W2.shape,self.W2))\n",
    "        print(\"Bias_2:{}\\n{}\".format(self.b2.shape,self.b2))\n",
    "        \n",
    "    def mini_batches(self,X, y, batch_size): \n",
    "        \"\"\"\n",
    "        Batching of training data as per the given batch size\n",
    "        Input: Training data\n",
    "        output: batched training data list\n",
    "        \"\"\"\n",
    "        self.batches=[]\n",
    "        data = np.hstack((X, y)) \n",
    "        np.random.shuffle(data) \n",
    "        n_batches = data.shape[0] \n",
    "        i = 0\n",
    "        \n",
    "        for i in range(n_batches + 1): \n",
    "            batch = data[i * batch_size:(i + 1)*batch_size, :] \n",
    "            X_mini = batch[:, :-4] \n",
    "            Y_mini = batch[:,-4:]\n",
    "            self.batches.append((X_mini, Y_mini)) \n",
    "        if data.shape[0] % batch_size != 0: \n",
    "            batch = data[i * batch_size:data.shape[0]] \n",
    "            X_mini = batch[:, :-4]\n",
    "            Y_mini = batch[:,-4:]\n",
    "            self.batches.append((X_mini, Y_mini)) \n",
    "        return self.batches \n",
    "    \n",
    "    def forwardPass(self,X):\n",
    "        \"\"\"\n",
    "        forward propagation through input layer to output layer\n",
    "        Input: Training features\n",
    "        output: predicted labels\n",
    "        \"\"\"\n",
    "        \n",
    "        # From Input layer to hidden layer\n",
    "        self.z1=np.dot(X,self.W1)+self.b1 # dot product of input and first set of weights\n",
    "        self.a1=sigmoid(self.z1) # First activation function result\n",
    "        \n",
    "        # Hidden layer to output layer\n",
    "        self.z2=np.dot(self.a1,self.W2)+self.b2 # dot product of output of hidden layer and second set of weights\n",
    "        self.a2=softmax(self.z2) # Output activation function\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def backProp(self,X,y_actual,y_pred):\n",
    "        \"\"\"\n",
    "        Backpropagation through output to input layer\n",
    "        Description:\n",
    "        Finds the gradient of cost function wrt parameters and adjust the parameters\n",
    "        \"\"\"\n",
    "        \n",
    "        # At output layer \n",
    "        \"\"\"\n",
    "        Gradient of cost function wrt w2: dJ_dw2 = dJ_da2 * da2_dz2 * dz2_dw2\n",
    "        Gradient of cost function wrt b2: dJ_db2 = dJ_da2 * da2_dz2 * dz2_db2\n",
    "        And by chain rule,\n",
    "        dJ_dz2 = dJ_da2 * da2_dz2 --> 1\n",
    "        \"\"\"\n",
    "        dJ_dz2 = y_pred - y_actual # Derivative of softmax function with cross-entropy loss\n",
    "        dz2_dw2 = self.a1  # Derivative of the o/p coming from hidden layers\n",
    "        dJ_dw2 = np.dot(dz2_dw2.T,dJ_dz2) # Gradient of cost function wrt w2\n",
    "        \n",
    "        dJ_db2 =  dJ_dz2\n",
    "        \n",
    "        # At Hidden layer\n",
    "        \"\"\"\n",
    "        Gradient of cost function wrt w1: dJ_dw1 = dJ_da1 * da1_dz1 * dz1_dw1\n",
    "        Gradient of cost function wrt b1: dJ_db1 = dJ_da1 * da1_dz1 * dz1_db1\n",
    "        And by chain rule,\n",
    "        dJ_da1 = dJ_dz2 * dz2_da1\n",
    "        from 1, we know dJ_dz2 = dJ_da2 * da2_dz2\n",
    "        \"\"\"\n",
    "        dz2_da1 = self.W2 # Derivative of \n",
    "        dJ_da1  = np.dot(dJ_dz2,dz2_da1.T)\n",
    "        da1_dz1 = sigmoid_der(self.z1)\n",
    "        \n",
    "        dz1_dw1 = X\n",
    "        dJ_dw1  = np.dot(dz1_dw1.T,da1_dz1*dJ_da1) \n",
    "        dJ_db1 = dJ_da1 * da1_dz1\n",
    "        \n",
    "        self.W1 -= self.learning_rate*dJ_dw1 # Adjusting the weights input layer --> hidden layer\n",
    "        self.b1 -= self.learning_rate*dJ_db1.sum(axis=0) # Adjusting the Bias input layer --> hidden layer\n",
    "        self.W2 -= self.learning_rate*dJ_dw2 # Adjusting the weights output layer\n",
    "        self.b2 -= self.learning_rate*dJ_db2.sum(axis=0) # Adjusting the Bias output layer\n",
    "    \n",
    "    def train(self,X,y,n_epochs,batch_size):\n",
    "        \"\"\"\n",
    "        Training the batched data: (ForwardPass + BackProp)\n",
    "        Input: Batched Train data, No of epochs, batch size and learning rate\n",
    "        \"\"\"\n",
    "        for itr in range(n_epochs):\n",
    "            batch_acc=[]\n",
    "            num_batch=self.mini_batches(X,y,batch_size)\n",
    "            for batch in num_batch:\n",
    "                if(len(batch[0])==0):\n",
    "                    break\n",
    "                X_b,y_b=batch\n",
    "                forward_out=self.forwardPass(X_b)\n",
    "                self.backProp(X_b,y_b,forward_out)\n",
    "                b = np.zeros_like(forward_out)\n",
    "                b[np.arange(len(forward_out)), forward_out.argmax(1)] = 1\n",
    "                train_accuracy = accuracy(y_b,b)\n",
    "                batch_acc.append(train_accuracy)\n",
    "            print(\"Epoch:: {} ; Training_acc= {}\\n\".format((itr+1),np.mean(batch_acc)))\n",
    "        self.pred=self.forwardPass(X)\n",
    "        one_hot_encoded = np.zeros_like(self.pred)\n",
    "        one_hot_encoded[np.arange(len(self.pred)), self.pred.argmax(1)] = 1\n",
    "      \n",
    "    def predict(self,X_test,y_test):\n",
    "        forward_out=self.forwardPass(X_test)\n",
    "        b = np.zeros_like(forward_out)\n",
    "        b[np.arange(len(forward_out)), forward_out.argmax(1)] = 1\n",
    "        self.test_accuracy = accuracy(y_test,b)\n",
    "        \n",
    "        return self.test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and preprocessing the data\n",
    "train_data=np.loadtxt(open(\"train_data.csv\", \"rb\"), delimiter=\",\")\n",
    "train_labels=np.loadtxt(open(\"train_labels.csv\",\"rb\"), delimiter=\",\")\n",
    "\n",
    "data_train=np.concatenate((train_data,train_labels),axis=1)\n",
    "np.random.shuffle(data_train)\n",
    "train_pro=int(len(data_train)*0.8)\n",
    "train, test = data_train[:train_pro,:], data_train[train_pro:,:]\n",
    "\n",
    "X_train=train[:,:784]\n",
    "X_test = test[:,:784]\n",
    "y_train=train[:,784:]\n",
    "y_test=test[:,784:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:: 1 ; Training_acc= 0.9066532258064516\n",
      "\n",
      "Epoch:: 2 ; Training_acc= 0.9607172192353644\n",
      "\n",
      "Epoch:: 3 ; Training_acc= 0.9644470579450417\n",
      "\n",
      "Epoch:: 4 ; Training_acc= 0.9676728643966547\n",
      "\n",
      "Epoch:: 5 ; Training_acc= 0.9692540322580645\n",
      "\n",
      "Epoch:: 6 ; Training_acc= 0.9696068548387097\n",
      "\n",
      "Epoch:: 7 ; Training_acc= 0.9709490740740739\n",
      "\n",
      "Epoch:: 8 ; Training_acc= 0.9717555256869772\n",
      "\n",
      "Epoch:: 9 ; Training_acc= 0.9730846774193549\n",
      "\n",
      "Epoch:: 10 ; Training_acc= 0.972795325567503\n",
      "\n",
      "Epoch:: 11 ; Training_acc= 0.9734001642771803\n",
      "\n",
      "Epoch:: 12 ; Training_acc= 0.9747983870967742\n",
      "\n",
      "Epoch:: 13 ; Training_acc= 0.9753528225806452\n",
      "\n",
      "Epoch:: 14 ; Training_acc= 0.9756048387096774\n",
      "\n",
      "Epoch:: 15 ; Training_acc= 0.9762918160095578\n",
      "\n",
      "Epoch:: 16 ; Training_acc= 0.9768966547192353\n",
      "\n",
      "Epoch:: 17 ; Training_acc= 0.9775014934289127\n",
      "\n",
      "Epoch:: 18 ; Training_acc= 0.9777217741935483\n",
      "\n",
      "Epoch:: 19 ; Training_acc= 0.9784274193548387\n",
      "\n",
      "Epoch:: 20 ; Training_acc= 0.9785786290322581\n",
      "\n",
      "Epoch:: 21 ; Training_acc= 0.9794354838709678\n",
      "\n",
      "Epoch:: 22 ; Training_acc= 0.9791647998805256\n",
      "\n",
      "Epoch:: 23 ; Training_acc= 0.9796370967741935\n",
      "\n",
      "Epoch:: 24 ; Training_acc= 0.9802923387096775\n",
      "\n",
      "Epoch:: 25 ; Training_acc= 0.980695564516129\n",
      "\n",
      "Epoch:: 26 ; Training_acc= 0.9808971774193549\n",
      "\n",
      "Epoch:: 27 ; Training_acc= 0.9810987903225806\n",
      "\n",
      "Epoch:: 28 ; Training_acc= 0.9811809289127836\n",
      "\n",
      "Epoch:: 29 ; Training_acc= 0.981332138590203\n",
      "\n",
      "Epoch:: 30 ; Training_acc= 0.9814516129032258\n",
      "\n",
      "Test_Accuracy: 0.9779842456069481\n"
     ]
    }
   ],
   "source": [
    "# Parameters for the model\n",
    "inputSize = X_train.shape[-1]\n",
    "hiddenSize = 10\n",
    "outSize = 4\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "n_epochs = 30\n",
    "\n",
    "#Build one hidden layer MLP\n",
    "MLP_oneLayer= MLP(inputSize,hiddenSize,outSize,lr)\n",
    "\n",
    "# Train the model\n",
    "MLP_oneLayer.train(X_train,y_train,n_epochs,batch_size)\n",
    "\n",
    "# Test the model\n",
    "prediction_acc=MLP_oneLayer.predict(X_test,y_test)\n",
    "\n",
    "print(\"Test_Accuracy:\",prediction_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlEnv]",
   "language": "python",
   "name": "conda-env-mlEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
